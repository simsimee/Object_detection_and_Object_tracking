안녕하세요 이번 Section4 프로젝트' 카메라를 이용한 자율주행’ 을 발표할 코드스테이츠 AI부트캠프 2기 심민우 입니다. 발표시작하겠습니다.

먼저 이런 자율주행으로 프로젝트를 정하게 된 것은 테슬라의 FSD 풀 셀프 드라이빙이나 우버, 구글의 웨이모와 같은 무인택시를 처음 봤을 때
와 이게된다고? 하는 신기함으로 굉장히 관심을 가지게 되었습니다. 물론 우버는 작년에 자율주행 사업에서 손을 떼기는 했지만 제가 후에 자율 주행과 관련된 일을 하고싶어서 프로젝트를 결정하게 되었습니다. 물론 라이다나 레이더 같은 다양한 센서를 통해서도 자율주행을 하지만 그 중에서 카메라를 이용한 자율주행을 구현해보려합니다.

사용한 데이터셋입니다. COCO dataset 이라는 Object detection에서 유명한 데이터셋인데요. 이 데이터셋으로 YOLO에 사전 훈련된 모델을 이용하였습니다. 결과로 보여줄 영상은 버클리 딥드라이브라는 사이트에서 받아온 도로 주행 영상으로 결과를 보여드릴 것입니다.

제가 이번 프로젝트에서 구현해본 기술은 물론 자율주행에서 사용하는 기술들은 더 많겠지만 객체 인식, 차선 검출, Object tracking 을 구현해보았습니다.

이번 프로젝트에서 사용한 라이브러리는 OpenCV 라이브러리와 아까 잠깐 말씀드렸던 yolo모델을 이용하였습니다.
OpenCV는 오픈 소스 컴퓨터 비전의 약자이고 텐서플로우나 파이토치같이 다양한 프레임워크를 지원하고 이미지 처리나 컴퓨터 비전에 중점을 두어 만들어진
라이브러리입니다. YOLO는 You Only Look Once의 약자로 말 그대로 이미지를 한 번만 본다는 특징이 있습니다. CNN같은 경우에 이미지를 여러개로 분할하여 분석하기때문에 이미지를 한 장만 Detection 하더라도 실제로는 여러장의 이미지를 분석하는데 반해 YOLO는 한 번에 보기 때문에 처리과정이 간단해서 빠르고,  class에 맥락적 이해도가 다른 모델에 비해서 높다는 장점이 있지만 작은 객체에 대해서는 정확도가 낮다는 단점이 있습니다. 실시간 영상을 분석해야하는데 있어서는 yolo의 빠르다는 장점이 매우 중요하다고 볼 수 있을 것 같습니다. 

차선 검출의 과정을 간단하게 소개해드리겠습니다. 내용이 조금 복잡해서 간단하게 순서와 목적 정도만 짚고 넘어가겠습니다. 일단 초기에 이미지에 있는 모든 선들을 인식하기위해서 원본 이미지에 grayscale을 해준 다음 노이즈를 제거해주고 부드럽게 해주는  GaussianBlur 라는 필터링을 거칩니다. 그 다음 Canny라는 엣지 검출 방법을 이용하여 이미지의 엣지들을 검출해줍니다.

그 다음은 모든 엣지 중에서도 원하는 범위의 엣지만 얻어내기 위해서 ROI 알오아이라는 Region of interest 관심영역을 지정해줍니다. 그 다음은 비트 와이즈 연산이라는 것을 통해서 원하는 부분을 마스킹한 후에 허프 변환이라는 방법으로 보시는 것처럼 앞의 차선만 검출하게 됩니다. 다음은 아까 했던 객체인식과 차선검출을 적용한 영상입니다. 

보시면 멀리 있는 선들은 검출을 하지않고 앞에 있는 선들에만 적용되어 선을 검출하는 것을 보실수 있고, 객체도 잘 인식하는 것을 보실 수 있습니다.

다음은 Object tracking입니다. 왼쪽은 object tracking의 순서도 입니다. 위에서 부터 영상이 들어오면 물체를 인식하고나서 무엇인지 분류하고 위치를 찍어주게 됩니다. 이를 합치면 object detection이 되고 detection된 박스들을 이전 프레임과 비교하면서 추적하는 것이 object tracking입니다.
이 방법을 통해서 차간거리유지같은 기능을 한 번 구현해보았습니다.

이 박스는 트래킹하고있는 박스구요. 이 박스가 물체를 인식하는 박스입니다. 물체를 인식한 박스말고 큰 박스 마진을 하나 추가해주었다. 
Detection 하는 박스와 만든 큰 박스 넓이 차를 이용해서 거리를 측정하고 일정 거리에 도달할때마다 문구가 다르게 뜨도록 되어있습니다.
가까워지면 Be careful, 멀어지면 speed up으로 문구가 뜨는 것을 확인하실 수 있습니다.

여기까지 yolo와 opencv를 이용해서 자율주행을 어느정도 구현해보았고 원래는 영상말고도 라즈베리파이라는 작은 컴퓨터에 카메라 센서를 이용해서 실시간 영상에 적용해보려고 했는데 환경적인 부분에서 이런저런 문제가 생겨버려서 잘안됐고, YOLO도 버전이 5까지 나왔는데 최신버전을 사용하지 못한 것이 아쉬웠습니다. 그리고 객체인식과 tracking을 하나의 영상에 동시에 적용을 못해서 이것들을 개선한다면 더 좋은 모델을 만들 수 있었을 것 같습니다.

감사합니다 ! 발표 마치겠습니다 !
